{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NKASG/Potato-Leaf-diesease-classifier/blob/main/plant%20diesease.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUnv17fCuRnP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras import models, layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UMnzyJH_-l8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = image_dataset_from_directory('/content/drive/MyDrive/Download',shuffle=True, image_size=(256,256), batch_size=32 )"
      ],
      "metadata": {
        "id": "-UfgEZgCuej3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = dataset.class_names\n",
        "class_names"
      ],
      "metadata": {
        "id": "XZBuBDYzXGRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "RNZiVeapX294"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of our data set"
      ],
      "metadata": {
        "id": "A0WkzOuMdgRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for image_batch, labels_batch in dataset.take(1):\n",
        "    for i in range(12):\n",
        "        ax = plt.subplot(3, 4, i + 1)\n",
        "        plt.imshow(image_batch[i].numpy().astype('uint8'))\n",
        "        plt.title(class_names[labels_batch[i]])\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "R0xtNUPHcTk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "i want to split the date into 3 categories which are training_data,validation_data,Test_data which are 80%, 10%, 10% respectively which by calculation with length of 682 the training_data = 546 , validation_data = 68 and Test_data = 68"
      ],
      "metadata": {
        "id": "b2Q502R5doWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_len = len(dataset)\n",
        "total_len"
      ],
      "metadata": {
        "id": "9Vxf1PfIcxBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_no = total_len * 0.8\n",
        "train_no"
      ],
      "metadata": {
        "id": "NfCLrgTof_83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_no = round(train_no)"
      ],
      "metadata": {
        "id": "8loKhZKBn5tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataset.take(train_no)\n",
        "len(train_ds)"
      ],
      "metadata": {
        "id": "NKiJVoPggysf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remain = dataset.skip(train_no)\n",
        "len(remain)"
      ],
      "metadata": {
        "id": "0dDrshTTiBFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_no = total_len * 0.1\n",
        "val_no"
      ],
      "metadata": {
        "id": "i-R9MVupj1bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_no = round(val_no)"
      ],
      "metadata": {
        "id": "MBx21j1zoph5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = remain.take(val_no)\n",
        "len(val_ds)"
      ],
      "metadata": {
        "id": "wFSyftYOkXWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = remain.skip(val_no)\n",
        "len(test_ds)"
      ],
      "metadata": {
        "id": "np-y1IqqlGI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prefetch and cache is used when loadingg batches while GPU is loading a batch the GPU will be loading the next batch its help saving time when reading those images while shuffle is needed for the image to be choosen randomly"
      ],
      "metadata": {
        "id": "yFjgCTVGmSKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "8QJwK8SkmQVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing of data by dividing each images by 255 and uniform resizing of the image to 255x255"
      ],
      "metadata": {
        "id": "MP_avTVBo_yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.Resizing(256,256),\n",
        "  layers.experimental.preprocessing.Rescaling(1./255),\n",
        "])"
      ],
      "metadata": {
        "id": "HQbi-1ISpVrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation this is needed when we want to address over fitting and to boost accuracy"
      ],
      "metadata": {
        "id": "cPabKEWHq1C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])"
      ],
      "metadata": {
        "id": "d77y_oFarLa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y)\n",
        ").prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "Ui46ZBQ1EIfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (32,256,256,3)"
      ],
      "metadata": {
        "id": "NqNr0W1iKSOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    Model = keras.models.Sequential([\n",
        "resize_and_rescale ,\n",
        "\n",
        "    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(3, activation='softmax'),\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "])\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "LQM-9EFr19A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " Model.build(input_shape)"
      ],
      "metadata": {
        "id": "s0HdBbAErMjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "C5ce67BEtmE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model.summary()"
      ],
      "metadata": {
        "id": "Bl3Aq4qxAMeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = Model.fit( train_ds, batch_size=32, validation_data=val_ds, verbose=1, epochs=5)"
      ],
      "metadata": {
        "id": "pNCs4d6BDRuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = Model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "d2oXAG2EwVzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "id": "nogKdVEcUI9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('accuracy', )\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['accuracy','val_acc'], loc= 'upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TawNlkPcUWZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['loss','val_loss'], loc= 'upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g8Ipv5q5UiOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model prediction"
      ],
      "metadata": {
        "id": "Si1KHouVwfkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "for images_batch, labels_batch in test_ds.take(1):\n",
        "\n",
        "    first_image = images_batch[0].numpy().astype('uint8')\n",
        "    first_label = labels_batch[0].numpy()\n",
        "\n",
        "    print(\"first image to predict\")\n",
        "    plt.imshow(first_image)\n",
        "    print(\"actual label:\",class_names[first_label])\n",
        "\n",
        "    batch_prediction = Model.predict(images_batch)\n",
        "    print(\"predicted label:\",class_names[np.argmax(batch_prediction[0])])"
      ],
      "metadata": {
        "id": "v72pbf8qySci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, img):\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "\n",
        "    predictions = model.predict(img_array)\n",
        "\n",
        "    predicted_class = class_names[np.argmax(predictions[0])]\n",
        "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
        "    return predicted_class, confidence"
      ],
      "metadata": {
        "id": "l05uEqomv-7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "for images, labels in test_ds.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "\n",
        "        predicted_class, confidence = predict(Model, images[i].numpy())\n",
        "        actual_class = class_names[labels[i]]\n",
        "\n",
        "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
        "\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "4WE_Bv2yveV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing Gradio"
      ],
      "metadata": {
        "id": "LjiEdDtkk2SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "# !pip install open_cv"
      ],
      "metadata": {
        "id": "WdkqWKmGkzQZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "DLtXCreypB6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python"
      ],
      "metadata": {
        "id": "TvxsJtoglkFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "ibJoZpqEl8BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(img):\n",
        "  img = cv2.resize(img,(256,256))\n",
        "  img_4d=img.reshape(-1,256,256,3)\n",
        "  prediction=Model.predict(img_4d).flatten()\n",
        "  return {class_names[i]: float(prediction[i]) for i in range(3)}"
      ],
      "metadata": {
        "id": "LOGolz2boTj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    signal = gr.Markdown(''' Welcome to Maize Classifier,This model can identify if a leaf is\n",
        "        **HEALTHY**, has **COMMON RUST**, **BLIGHT** or **GRAY LEAF SPOT**''')\n",
        "    with gr.Row():\n",
        "        inp = gr.Image()\n",
        "        out = gr.Label()\n",
        "        inp.upload(fn= predict_image, inputs = inp, outputs = out)"
      ],
      "metadata": {
        "id": "AhsgQ1u0jd4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()"
      ],
      "metadata": {
        "id": "xeotnNTOj7zY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model.save(\"/content/sample_data\")\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"/content/sample_data\")\n",
        "tflite_model = converter.convert()"
      ],
      "metadata": {
        "id": "xrDFB1EIE1Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "model_version=max([int(i) for i in os.listdir(\"../models\") + [0]])+1\n",
        "model.save(f\"../models/{model_version}\")"
      ],
      "metadata": {
        "id": "yT6wPk4H7FOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"/content/sample_data\")\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "with open(\"tflite_quant_model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_quant_model)"
      ],
      "metadata": {
        "id": "xYAhEhBlWotS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}